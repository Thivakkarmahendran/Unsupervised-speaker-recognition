\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage[table,xcdraw]{xcolor}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Unsupervised Speaker Identification}

\author{Thivakkar Mahendran\\
University of Massachusetts Amherst\\
%Institution1 address\\
{\tt\small tmahendran@umass.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Varun Prasad\\
University of Massachusetts Amherst\\
%First line of institution2 address\\
{\tt\small varunanantha@umass.edu}
}

\maketitle
%\thispagestyle{empty}



%%%%%%%%% BODY TEXT
\section{Introduction}

A voice recorder makes it super convenient to take notes or preserve the minutes of a meeting. In order to make the experience better, there are tools that can be used to automatically convert speech to text. One area where this tool currently fails at is identifying the speaker. The problem that we are trying to solve is to identify the speaker. Our ultimate goal is to build a project where we are able to record a meeting and identify which person is speaking at that specific time and what they said.

It would be cumbersome and time consuming to record each speaker’s voice beforehand and train a speaker recognition model on the speakers’ voice. We want this tool to predict the speaker without prior training of the speaker’s voice. This would be an unsupervised learning project. \hfill \break \break
For example, if two speakers were talking in a meeting we would like out tool to output like:
\\  {\bf Speaker1}: Hey, How are you? 
\\ {\bf Speaker2}: I’m doing well.
\\ {\bf Speaker2}: I was able to complete the tasks that we talked about last week
\\ {\bf Speaker1}: That’s great.
\\ These two speaker’s voices have not been trained before but still the program would identify the two speakers. 

%------------------------------------------------------------------------

\section{Problem Statement}

\subsection{Dataset}

The dataset that we planned to use was VoxCeleb, which is a  large scale audio-visual dataset of human speech. The dataset contains 7,000+ speakers, 1 million+ utterances, and 2,000+ hours of audio. The total size of the dataset was around 250 GB. The dataset was really huge in terms of computational complexity and also space required to store and train the model. After multiple attempts at trying to use the VoxCeleb dataset, we decided to build our own dataset. 

We decided to create our own dataset so we could tailor the dataset to exactly suit our project. We built a pipeline to scrape audio from YouTube videos, and then split a whole audio into chunks of 1 second audio clips. We decided to create audio clips of length 1 second because when this model is getting used in the real world, we want to recognize the speaker instantly with as little of a delay as possible. According to (1) virtualspeech.com, an average person speaks about 150 words per minute, which is about 2.5 words per second. This is enough to extract useful features from the person’s speech. At the end of the project, we are planning to experiment on the duration of each audio clip to see its effect on the accuracy of the model. 

The YouTube videos that we chose to include in our dataset were speeches/monologues from celebrities. We thought this would be the best way to build a labeled dataset of audios of different people. To start with, the dataset includes 7  celebrities (Obama, Hillary, Ivanka, Trump, Modi, Xi-Jinping, and Chadwick-Boseman) and one “No Speaker” class which includes multiple background noises. The “No Speaker” class is included so that the model can recognize when no one is speaking.  We wanted the dataset to be as diverse as possible, which is why we included speakers of both genders, different races, and also different languages. The current dataset has a size of 4.1+ hours.  

\begin{table}[h]
    \begin{tabular}{l|l|l|l|l|}
    \cline{2-5}
                                                    & \textbf{Gender} & \textbf{Language} & \textbf{Race} & \textbf{Length} \\ \hline
    \multicolumn{1}{|l|}{\textbf{Obama}}            & Male            & English           & Black         & 19.5 mins       \\ \hline
    \multicolumn{1}{|l|}{\textbf{Hillary}}          & Female          & English           & White         & 57.5 mins       \\ \hline
    \multicolumn{1}{|l|}{\textbf{Ivanka}}           & Female          & English           & White         & 17.9 mins       \\ \hline
    \multicolumn{1}{|l|}{\textbf{Trump}}            & Male            & English           & White         & 41.6 mins       \\ \hline
    \multicolumn{1}{|l|}{\textbf{Modi}}             & Male            & Hindi             & Indian        & 32.4 mins       \\ \hline
    \multicolumn{1}{|l|}{\textbf{Xi-Jinping}}       & Male            & Chinese           & Chinese       & 11.18 mins      \\ \hline
    \multicolumn{1}{|l|}{\textbf{Chadwick-Boseman}} & Male            & English           & Black         & 27.1 mins       \\ \hline
    \multicolumn{1}{|l|}{\textbf{No Speaker}}       & N/A             & N/A               & N/A           & 39.6 mins       \\ \hline
    \end{tabular}
    \caption{Details of the dataset}
    \label{Dataset}
\end{table}

We are planning to add more diverse speakers to make the model predict better. We will experiment with the dataset to see the impact on the model. For example, we will train the model on all male voices or only English as the language to see the impact on the model. 

\subsection{Expected Result and Evaluation}

The project is split into two parts, the first part would be training the model to recognize and predict the 8 different speakers from the dataset. The second part would be to modify the model to recognize different speakers who have not been trained before.  We want the model to differentiate different speakers and label each speaker as “speaker\#”. A python script has been created to get a live stream of audio data from the computer, process every 1 second audio clip by extracting features on the audio and then using the features to predict the speaker by using the model. So every 1 second, the program will output the current speaker or “no speaker” if no one is talking.   

For the first part of the project we will be performing the five-fold cross validation to train and evaluate the model. We will be evaluating the model by getting the accuracy, precision, and recall. We would like the model to have at least 90\% accuracy on the test dataset so the model would work better in the second part of the project. 

Evaluating the second part of the project would be slightly harder because we don’t have a labelled dataset of speakers talking at every timestamp. We either need to build our own dataset to make it work for this specific use case or we have to manually evaluate the model. One possible way to evaluate is to play an interview video between two or more people and physically check if the model predicts the speaker speaking at that very point of time. 

We are expecting several factors to impact the accuracy of the model. For example, the speaker’s gender, age, race, language, dialect of a language could affect the accuracy of the model. Multiple speakers speaking at the same time might also impact the accuracy of the model. These are some things that we are interested in evaluating. These results would help us improve the model in the future based on its weakness. 

%------------------------------------------------------------------------

\section{Technical Approach}

Real-world applications of such a system would require an unsupervised learning approach to the problem since it is extremely difficult to re-train the model with the voices of all the speakers in every particular situation, and also there would be no labels to train the model on. It is also difficult to use an entirely unsupervised approach during the training phase because there would be absolutely no way to know whether the model is trained properly i.e. evaluation of the model would become almost impossible.

Because of this, we have chosen to go with a hybrid approach where we train the model using a supervised approach over labelled data, and then use an unsupervised approach for the final classification. The first phase will be done by building a fully-connected neural network and training over the 8-speaker dataset specified in section 2.1, which will learn to classify the different speakers. The second phase can then be implemented by cutting off the final classification layer of the neural net, and considering the embeddings of the penultimate layer: The embeddings at every interval can be compared to previous embeddings using some distance metric and a classification method, like k-nearest neighbors, can be used to decide if the speaker is the same or not.

Feature selection is an important part of any deep learning project. Because we have audio data for this project, we have chosen features like 
\begin{itemize}
    \item MFCC (Mel-Frequency Cepstral Coefficients): coefficients used to detect the envelope of audio signals. Sounds produced by humans can be accurately represented by determining the envelope of the speech signal.
    \item Zero-crossing rate: the number of times in a given time interval/frame that the amplitude of the speech signals passes through a value of zero. This feature is used to distinguish between periods of voiced and unvoiced sounds.
    \item Spectral roll off, is a measure of the amount of the right-skewedness of the power spectrum. That is, the roll-off point is the frequency below which 85\% of accumulated spectral magnitude is concentrated.
\end{itemize}

We are continuing to experiment with various other features that are useful in audio processing, like spectral entropy, pitch and spectral flux, to see if the model would benefit from including any of them. We are also experimenting with other classification methods that can be used instead of k-nearest neighbors which might allow the model to better distinguish between different speakers.
%------------------------------------------------------------------------


\section{Preliminary Results}

The classes for the confusion matrix below are: Obama(1), Hillary(2), Ivanka(3), Trump(4), No Speaker(5), Modi(6), Xi-Jinping(7), and Chadwick-Boseman(8).

\begin{table}[h]
    \begin{tabular}{l|l|l|l|l|l|l|l|l|}
    \cline{2-9}
                                     & \textbf{1}                    & \textbf{2}                    & \textbf{3}                    & \textbf{4}                    & 5                             & \textbf{6}                    & \textbf{7}                    & \textbf{8}                    \\ \hline
    \multicolumn{1}{|l|}{\textbf{1}} & \cellcolor[HTML]{67FD9A}0.735 & 0.016                         & 0.033                         & 0.022                         & 0.000                         & 0.010                         & 0.058                         & 0.124                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{2}} & 0.017                         & \cellcolor[HTML]{67FD9A}0.924 & 0.000                         & 0.001                         & 0.003                         & 0.055                         & 0.000                         & 0.001                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{3}} & 0.010                         & 0.000                         & \cellcolor[HTML]{67FD9A}0.988 & 0.002                         & 0.000                         & 0.000                         & 0.000                         & 0.000                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{4}} & 0.014                         & 0.006                         & 0.002                         & \cellcolor[HTML]{67FD9A}0.805 & 0.000                         & 0.166                         & 0.006                         & 0.000                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{5}} & 0.000                         & 0.757                         & 0.000                         & 0.000                         & \cellcolor[HTML]{67FD9A}0.243 & 0.000                         & 0.000                         & 0.000                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{6}} & 0.004                         & 0.079                         & 0.000                         & 0.081                         & 0.000                         & \cellcolor[HTML]{67FD9A}0.832 & 0.002                         & 0.002                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{7}} & 0.001                         & 0.000                         & 0.000                         & 0.001                         & 0.003                         & 0.000                         & \cellcolor[HTML]{67FD9A}0.945 & 0.049                         \\ \hline
    \multicolumn{1}{|l|}{\textbf{8}} & 0.004                         & 0.000                         & 0.000                         & 0.000                         & 0.001                         & 0.000                         & 0.004                         & \cellcolor[HTML]{67FD9A}0.991 \\ \hline
    \end{tabular}
    \caption{Confusion matrix}
    \label{Confusion matrix}
\end{table}


%------------------------------------------------------------------------


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
