{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "#audio feature extraction library\n",
    "import librosa\n",
    "from librosa import feature\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#model\n",
    "import tensorflow as tf\n",
    "from spela.spectrogram import Spectrogram \n",
    "from spela.melspectrogram import Melspectrogram\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllAudioFilePaths():\n",
    "    audioFilesPaths = [y for x in os.walk(\"Dataset/Youtube Speech Dataset/Dataset\") for y in glob(os.path.join(x[0], '*.wav'))]\n",
    "    return audioFilesPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [\"Obama\", \"Hillary\", \"Ivanka\", \"Trump\", \"No Speaker\", \"Modi\", \"Xi-Jinping\", \"Chadwick-Boseman\"]\n",
    "\n",
    "def speakerToLabel(speakerName):\n",
    "    index = speakers.index(speakerName)\n",
    "    \n",
    "    if(index == -1):\n",
    "        print(\"Speaker not found: {}\".format(speakerName))\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeakerAndAudio(audioPaths):\n",
    "    audio_Paths = []\n",
    "    labels = []\n",
    "    uniqueSpeakers = {}\n",
    "\n",
    "    for audioPath in audioPaths:\n",
    "        speakerName = audioPath.split(\"/\")[3]\n",
    "\n",
    "        audioLength = librosa.get_duration(filename=audioPath)\n",
    "        \n",
    "        if audioLength == 1.0:\n",
    "            audio_Paths.append(audioPath)\n",
    "            labels.append(speakerToLabel(speakerName))\n",
    "            uniqueSpeakers[speakerName] = uniqueSpeakers.get(speakerName, 0) + 1\n",
    "        else:\n",
    "            print(\"Audio clip discarded, actual length = {}\".format(audioLength))\n",
    "    \n",
    "    return audio_Paths, labels, uniqueSpeakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " #https://towardsdatascience.com/how-i-understood-what-features-to-consider-while-training-audio-files-eedfb6e9002b\n",
    "        \n",
    "\n",
    "def getFeatures(audio_Paths):\n",
    "    \n",
    "    data_X = []\n",
    "    \n",
    "    for path in audio_Paths:\n",
    "        \n",
    "        audioFeatureArray = []        \n",
    "        y, sr = librosa.load(path)\n",
    "        \n",
    "        #mfcc\n",
    "        mfccArray = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        audioFeatureArray.append(mfccArray.flatten())\n",
    "        audioFeatureNumpyArray = np.array(audioFeatureArray)\n",
    "        \n",
    "        \n",
    "        #zero_crossing_rate\n",
    "        zeroCrossingArray = librosa.feature.zero_crossing_rate(y=y)\n",
    "        np.append(audioFeatureNumpyArray, zeroCrossingArray.flatten())\n",
    "        \n",
    "        #spectral_rolloff\n",
    "        spectralRollOffArray = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        np.append(audioFeatureNumpyArray, spectralRollOffArray.flatten())\n",
    "        \n",
    "        data_X.append(audioFeatureNumpyArray.flatten())\n",
    "        \n",
    "        \n",
    "    return data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio clip discarded, actual length = 0.221\n",
      "Audio clip discarded, actual length = 0.4819954648526077\n",
      "Audio clip discarded, actual length = 0.66\n",
      "Audio clip discarded, actual length = 0.3\n",
      "Audio clip discarded, actual length = 0.6059863945578231\n",
      "Audio clip discarded, actual length = 0.12598639455782312\n",
      "Audio clip discarded, actual length = 0.7489795918367347\n",
      "\n",
      "-------------------------\n",
      "Speaker: No Speaker, length: 9.65 minutes\n",
      "Speaker: Hillary, length: 57.53333333333333 minutes\n",
      "Speaker: Ivanka, length: 17.916666666666668 minutes\n",
      "Speaker: Xi-Jinping, length: 11.183333333333334 minutes\n",
      "Speaker: Modi, length: 32.4 minutes\n",
      "Speaker: Chadwick-Boseman, length: 27.083333333333332 minutes\n",
      "Speaker: Obama, length: 19.466666666666665 minutes\n",
      "Speaker: Trump, length: 41.6 minutes\n",
      "-------------------------\n",
      "\n",
      "Total Dataset size: 13010\n",
      "Time to extract Features 11.66484010219574 minutes\n",
      "X data: 13010\n",
      "Y data: 13010\n"
     ]
    }
   ],
   "source": [
    "audioPaths = findAllAudioFilePaths()\n",
    "audio_Paths, labels, uniqueSpeakers = getSpeakerAndAudio(audioPaths)\n",
    " \n",
    "    \n",
    "#print(\"Speakers: {}\".format(uniqueSpeakers))\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------------------\")\n",
    "for speaker in uniqueSpeakers: \n",
    "    print(\"Speaker: {}, length: {} minutes\".format(speaker, int(uniqueSpeakers[speaker])/60 )) \n",
    "print(\"-------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print(\"Total Dataset size: {}\".format(len(audio_Paths)))\n",
    "\n",
    "startTime = time.time()\n",
    "data_X = getFeatures(audio_Paths)\n",
    "endTime = time.time()\n",
    "\n",
    "print(\"Time to extract Features {} minutes\".format( (endTime-startTime)/60))\n",
    "\n",
    "print(\"X data: {}\".format(len(data_X)))\n",
    "print(\"Y data: {}\".format(len(labels)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, labels, test_size=0.2)\n",
    "\n",
    "train_x = np.array(train_X)\n",
    "train_y = np.array(train_Y)\n",
    "test_x = np.array(test_X)\n",
    "test_y = np.array(test_Y)\n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(train_y)\n",
    "test_y = tf.keras.utils.to_categorical(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "#from tf.keras.layers import Dense\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(12,input_shape= train_x.shape, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(len(speakers), activation=\"softmax\"))\n",
    "                          \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4)\n",
    "            , loss = \"categorical_crossentropy\"\n",
    "            , metrics = [\"accuracy\"])\n",
    "                          \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 10408, 12)         10572     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10408, 8)          104       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10408, 8)          72        \n",
      "=================================================================\n",
      "Total params: 10,748\n",
      "Trainable params: 10,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10408, 880) for input Tensor(\"dense_18_input:0\", shape=(None, 10408, 880), dtype=float32), but it was called on an input with incompatible shape (None, 880).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10408, 880) for input Tensor(\"dense_18_input:0\", shape=(None, 10408, 880), dtype=float32), but it was called on an input with incompatible shape (None, 880).\n",
      "309/326 [===========================>..] - ETA: 0s - loss: 3.1903 - accuracy: 0.2244WARNING:tensorflow:Model was constructed with shape (None, 10408, 880) for input Tensor(\"dense_18_input:0\", shape=(None, 10408, 880), dtype=float32), but it was called on an input with incompatible shape (None, 880).\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 3.1337 - accuracy: 0.2272 - val_loss: 2.0595 - val_accuracy: 0.2759\n",
      "Epoch 2/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 2.0055 - accuracy: 0.2635 - val_loss: 1.8084 - val_accuracy: 0.2763\n",
      "Epoch 3/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.7305 - accuracy: 0.3770 - val_loss: 1.6155 - val_accuracy: 0.4162\n",
      "Epoch 4/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.6402 - accuracy: 0.4194 - val_loss: 1.5617 - val_accuracy: 0.4343\n",
      "Epoch 5/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.5771 - accuracy: 0.4357 - val_loss: 1.5109 - val_accuracy: 0.4516\n",
      "Epoch 6/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.5028 - accuracy: 0.4609 - val_loss: 1.3764 - val_accuracy: 0.5238\n",
      "Epoch 7/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.2799 - accuracy: 0.5693 - val_loss: 1.2170 - val_accuracy: 0.5807\n",
      "Epoch 8/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.1491 - accuracy: 0.6053 - val_loss: 1.1168 - val_accuracy: 0.6211\n",
      "Epoch 9/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.0956 - accuracy: 0.6209 - val_loss: 1.0339 - val_accuracy: 0.6372\n",
      "Epoch 10/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.0360 - accuracy: 0.6407 - val_loss: 1.0274 - val_accuracy: 0.6641\n",
      "Epoch 11/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.9835 - accuracy: 0.6653 - val_loss: 1.0054 - val_accuracy: 0.6676\n",
      "Epoch 12/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.9426 - accuracy: 0.6728 - val_loss: 0.9456 - val_accuracy: 0.6791\n",
      "Epoch 13/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.9323 - accuracy: 0.6751 - val_loss: 0.9589 - val_accuracy: 0.6852\n",
      "Epoch 14/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.8970 - accuracy: 0.6866 - val_loss: 0.9388 - val_accuracy: 0.6883\n",
      "Epoch 15/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8750 - accuracy: 0.6921 - val_loss: 0.9085 - val_accuracy: 0.6887\n",
      "Epoch 16/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8707 - accuracy: 0.6900 - val_loss: 0.9001 - val_accuracy: 0.6872\n",
      "Epoch 17/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8536 - accuracy: 0.6931 - val_loss: 0.8758 - val_accuracy: 0.6914\n",
      "Epoch 18/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8663 - accuracy: 0.6875 - val_loss: 0.9035 - val_accuracy: 0.6972\n",
      "Epoch 19/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8232 - accuracy: 0.6995 - val_loss: 0.9476 - val_accuracy: 0.6875\n",
      "Epoch 20/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.8204 - accuracy: 0.6987 - val_loss: 0.8650 - val_accuracy: 0.7006\n",
      "Epoch 21/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8059 - accuracy: 0.6992 - val_loss: 0.8750 - val_accuracy: 0.6818\n",
      "Epoch 22/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7985 - accuracy: 0.6986 - val_loss: 0.8684 - val_accuracy: 0.6799\n",
      "Epoch 23/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7817 - accuracy: 0.7039 - val_loss: 0.8292 - val_accuracy: 0.6987\n",
      "Epoch 24/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.7025 - val_loss: 0.8383 - val_accuracy: 0.7029\n",
      "Epoch 25/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7654 - accuracy: 0.7073 - val_loss: 0.8361 - val_accuracy: 0.6933\n",
      "Epoch 26/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.7678 - accuracy: 0.7045 - val_loss: 0.8281 - val_accuracy: 0.6937\n",
      "Epoch 27/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8116 - accuracy: 0.6949 - val_loss: 0.8556 - val_accuracy: 0.6945\n",
      "Epoch 28/50\n",
      "326/326 [==============================] - 0s 2ms/step - loss: 0.7593 - accuracy: 0.7083 - val_loss: 0.8301 - val_accuracy: 0.7041\n",
      "Epoch 29/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7476 - accuracy: 0.7109 - val_loss: 0.8437 - val_accuracy: 0.6872\n",
      "Epoch 30/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7503 - accuracy: 0.7083 - val_loss: 0.8389 - val_accuracy: 0.6849\n",
      "Epoch 31/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7399 - accuracy: 0.7093 - val_loss: 0.8147 - val_accuracy: 0.6949\n",
      "Epoch 32/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7437 - accuracy: 0.7068 - val_loss: 0.8133 - val_accuracy: 0.6852\n",
      "Epoch 33/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7357 - accuracy: 0.7083 - val_loss: 0.8120 - val_accuracy: 0.7037\n",
      "Epoch 34/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7528 - accuracy: 0.7051 - val_loss: 0.8449 - val_accuracy: 0.6922\n",
      "Epoch 35/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7272 - accuracy: 0.7116 - val_loss: 0.8113 - val_accuracy: 0.6949\n",
      "Epoch 36/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7219 - accuracy: 0.7102 - val_loss: 0.7951 - val_accuracy: 0.6960\n",
      "Epoch 37/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7128 - accuracy: 0.7127 - val_loss: 0.7937 - val_accuracy: 0.7006\n",
      "Epoch 38/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7214 - accuracy: 0.7100 - val_loss: 0.8087 - val_accuracy: 0.7002\n",
      "Epoch 39/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7081 - accuracy: 0.7136 - val_loss: 0.7995 - val_accuracy: 0.7006\n",
      "Epoch 40/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7452 - accuracy: 0.7053 - val_loss: 0.8745 - val_accuracy: 0.6925\n",
      "Epoch 41/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7171 - accuracy: 0.7157 - val_loss: 0.8075 - val_accuracy: 0.6956\n",
      "Epoch 42/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7005 - accuracy: 0.7167 - val_loss: 0.7883 - val_accuracy: 0.6983\n",
      "Epoch 43/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6934 - accuracy: 0.7180 - val_loss: 0.7914 - val_accuracy: 0.7060\n",
      "Epoch 44/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6982 - accuracy: 0.7169 - val_loss: 0.7972 - val_accuracy: 0.7052\n",
      "Epoch 45/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.7191 - val_loss: 0.8031 - val_accuracy: 0.7045\n",
      "Epoch 46/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.7161 - val_loss: 0.7808 - val_accuracy: 0.7018\n",
      "Epoch 47/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.7191 - val_loss: 0.7836 - val_accuracy: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6794 - accuracy: 0.7178 - val_loss: 0.7850 - val_accuracy: 0.7025\n",
      "Epoch 49/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6875 - accuracy: 0.7156 - val_loss: 0.8148 - val_accuracy: 0.6998\n",
      "Epoch 50/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6715 - accuracy: 0.7239 - val_loss: 0.7865 - val_accuracy: 0.7014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea4c970b50>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=50, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 10408, 12)         10572     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10408, 8)          104       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10408, 8)          72        \n",
      "=================================================================\n",
      "Total params: 10,748\n",
      "Trainable params: 10,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
