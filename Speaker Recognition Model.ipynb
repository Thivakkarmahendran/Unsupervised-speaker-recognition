{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "#audio feature extraction library\n",
    "import librosa\n",
    "from librosa import feature\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#model\n",
    "import tensorflow as tf\n",
    "from spela.spectrogram import Spectrogram \n",
    "from spela.melspectrogram import Melspectrogram\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllAudioFilePaths():\n",
    "    audioFilesPaths = [y for x in os.walk(\"Dataset/Youtube Speech Dataset/Dataset\") for y in glob(os.path.join(x[0], '*.wav'))]\n",
    "    return audioFilesPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [\"Obama\", \"Hillary\", \"Ivanka\", \"Trump\", \"No Speaker\", \"Modi\", \"Xi-Jinping\", \"Chadwick-Boseman\"]\n",
    "\n",
    "def speakerToLabel(speakerName):\n",
    "    index = speakers.index(speakerName)\n",
    "    \n",
    "    if(index == -1):\n",
    "        print(\"Speaker not found: {}\".format(speakerName))\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeakerAndAudio(audioPaths):\n",
    "    audio_Paths = []\n",
    "    labels = []\n",
    "    uniqueSpeakers = {}\n",
    "\n",
    "    for audioPath in audioPaths:\n",
    "        speakerName = audioPath.split(\"/\")[3]\n",
    "\n",
    "        audioLength = librosa.get_duration(filename=audioPath)\n",
    "        \n",
    "        if audioLength == 1.0:\n",
    "            audio_Paths.append(audioPath)\n",
    "            labels.append(speakerToLabel(speakerName))\n",
    "            uniqueSpeakers[speakerName] = uniqueSpeakers.get(speakerName, 0) + 1\n",
    "        else:\n",
    "            print(\"Audio clip discarded, actual length = {}\".format(audioLength))\n",
    "    \n",
    "    return audio_Paths, labels, uniqueSpeakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " #https://towardsdatascience.com/how-i-understood-what-features-to-consider-while-training-audio-files-eedfb6e9002b\n",
    "        \n",
    "\n",
    "def getFeatures(audio_Paths):\n",
    "    \n",
    "    data_X = []\n",
    "    \n",
    "    for path in audio_Paths:\n",
    "        \n",
    "        audioFeatureArray = []        \n",
    "        y, sr = librosa.load(path)\n",
    "        \n",
    "        #mfcc\n",
    "        mfccArray = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        audioFeatureArray.append(mfccArray.flatten())\n",
    "        audioFeatureNumpyArray = np.array(audioFeatureArray)\n",
    "        \n",
    "        \n",
    "        #zero_crossing_rate\n",
    "        zeroCrossingArray = librosa.feature.zero_crossing_rate(y=y)\n",
    "        np.append(audioFeatureNumpyArray, zeroCrossingArray.flatten())\n",
    "        \n",
    "        #spectral_rolloff\n",
    "        spectralRollOffArray = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        np.append(audioFeatureNumpyArray, spectralRollOffArray.flatten())\n",
    "        \n",
    "        data_X.append(audioFeatureNumpyArray.flatten())\n",
    "        \n",
    "        \n",
    "    return data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio clip discarded, actual length = 0.221\n",
      "Audio clip discarded, actual length = 0.4819954648526077\n",
      "Audio clip discarded, actual length = 0.66\n",
      "Audio clip discarded, actual length = 0.3\n",
      "Audio clip discarded, actual length = 0.6059863945578231\n",
      "Audio clip discarded, actual length = 0.12598639455782312\n",
      "Audio clip discarded, actual length = 0.7489795918367347\n",
      "\n",
      "-------------------------\n",
      "Speaker: No Speaker, length: 9.65 minutes\n",
      "Speaker: Hillary, length: 57.53333333333333 minutes\n",
      "Speaker: Ivanka, length: 17.916666666666668 minutes\n",
      "Speaker: Xi-Jinping, length: 11.183333333333334 minutes\n",
      "Speaker: Modi, length: 32.4 minutes\n",
      "Speaker: Chadwick-Boseman, length: 27.083333333333332 minutes\n",
      "Speaker: Obama, length: 19.466666666666665 minutes\n",
      "Speaker: Trump, length: 41.6 minutes\n",
      "-------------------------\n",
      "\n",
      "Total Dataset size: 13010\n",
      "Time to extract Features 14.542151816685994 minutes\n",
      "X data: 13010\n",
      "Y data: 13010\n"
     ]
    }
   ],
   "source": [
    "audioPaths = findAllAudioFilePaths()\n",
    "audio_Paths, labels, uniqueSpeakers = getSpeakerAndAudio(audioPaths)\n",
    " \n",
    "    \n",
    "#print(\"Speakers: {}\".format(uniqueSpeakers))\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------------------\")\n",
    "for speaker in uniqueSpeakers: \n",
    "    print(\"Speaker: {}, length: {} minutes\".format(speaker, int(uniqueSpeakers[speaker])/60 )) \n",
    "print(\"-------------------------\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print(\"Total Dataset size: {}\".format(len(audio_Paths)))\n",
    "\n",
    "startTime = time.time()\n",
    "data_X = getFeatures(audio_Paths)\n",
    "endTime = time.time()\n",
    "\n",
    "print(\"Time to extract Features {} minutes\".format( (endTime-startTime)/60))\n",
    "\n",
    "print(\"X data: {}\".format(len(data_X)))\n",
    "print(\"Y data: {}\".format(len(labels)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(data_X, labels, test_size=0.2)\n",
    "\n",
    "train_x = np.array(train_X)\n",
    "train_y = np.array(train_Y)\n",
    "test_x = np.array(test_X)\n",
    "test_y = np.array(test_Y)\n",
    "\n",
    "train_y = tf.keras.utils.to_categorical(train_y)\n",
    "test_y = tf.keras.utils.to_categorical(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "#from tf.keras.layers import Dense\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(12,input_shape= train_x.shape, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(len(speakers), activation=\"softmax\"))\n",
    "                          \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-4)\n",
    "            , loss = \"categorical_crossentropy\"\n",
    "            , metrics = [\"accuracy\"])\n",
    "                          \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10408, 880) for input Tensor(\"dense_4_input:0\", shape=(None, 10408, 880), dtype=float32), but it was called on an input with incompatible shape (None, 880).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10408, 880) for input Tensor(\"dense_4_input:0\", shape=(None, 10408, 880), dtype=float32), but it was called on an input with incompatible shape (None, 880).\n",
      "278/326 [========================>.....] - ETA: 0s - loss: 3.9513 - accuracy: 0.2769WARNING:tensorflow:Model was constructed with shape (None, 10408, 880) for input Tensor(\"dense_4_input:0\", shape=(None, 10408, 880), dtype=float32), but it was called on an input with incompatible shape (None, 880).\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 3.6320 - accuracy: 0.2847 - val_loss: 1.7161 - val_accuracy: 0.3578\n",
      "Epoch 2/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.5216 - accuracy: 0.4422 - val_loss: 1.3472 - val_accuracy: 0.5004\n",
      "Epoch 3/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 1.1325 - accuracy: 0.6414 - val_loss: 1.0392 - val_accuracy: 0.6756\n",
      "Epoch 4/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.9654 - accuracy: 0.6945 - val_loss: 0.9482 - val_accuracy: 0.7060\n",
      "Epoch 5/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8976 - accuracy: 0.7082 - val_loss: 0.9493 - val_accuracy: 0.7191\n",
      "Epoch 6/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.8367 - accuracy: 0.7203 - val_loss: 0.8409 - val_accuracy: 0.7260\n",
      "Epoch 7/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7947 - accuracy: 0.7274 - val_loss: 0.8272 - val_accuracy: 0.7271\n",
      "Epoch 8/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7739 - accuracy: 0.7298 - val_loss: 0.7765 - val_accuracy: 0.7341\n",
      "Epoch 9/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7291 - accuracy: 0.7382 - val_loss: 0.7676 - val_accuracy: 0.7306\n",
      "Epoch 10/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.7085 - accuracy: 0.7456 - val_loss: 0.7072 - val_accuracy: 0.7629\n",
      "Epoch 11/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6441 - accuracy: 0.7803 - val_loss: 0.6801 - val_accuracy: 0.7821\n",
      "Epoch 12/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.6077 - accuracy: 0.7899 - val_loss: 0.6467 - val_accuracy: 0.7852\n",
      "Epoch 13/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.5413 - accuracy: 0.7948 - val_loss: 0.5205 - val_accuracy: 0.8028\n",
      "Epoch 14/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.8200 - val_loss: 0.4975 - val_accuracy: 0.8105\n",
      "Epoch 15/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8628 - val_loss: 0.3594 - val_accuracy: 0.9162\n",
      "Epoch 16/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.9057 - val_loss: 0.5122 - val_accuracy: 0.8786\n",
      "Epoch 17/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.2737 - accuracy: 0.9216 - val_loss: 0.2696 - val_accuracy: 0.9412\n",
      "Epoch 18/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.2191 - accuracy: 0.9375 - val_loss: 0.2704 - val_accuracy: 0.9335\n",
      "Epoch 19/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9407 - val_loss: 0.2465 - val_accuracy: 0.9404\n",
      "Epoch 20/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9503 - val_loss: 0.2415 - val_accuracy: 0.9431\n",
      "Epoch 21/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9564 - val_loss: 0.2462 - val_accuracy: 0.9385\n",
      "Epoch 22/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9652 - val_loss: 0.2006 - val_accuracy: 0.9543\n",
      "Epoch 23/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9637 - val_loss: 0.1884 - val_accuracy: 0.9612\n",
      "Epoch 24/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9751 - val_loss: 0.2092 - val_accuracy: 0.9635\n",
      "Epoch 25/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9737 - val_loss: 0.1793 - val_accuracy: 0.9631\n",
      "Epoch 26/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9771 - val_loss: 0.1634 - val_accuracy: 0.9666\n",
      "Epoch 27/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9775 - val_loss: 0.2192 - val_accuracy: 0.9508\n",
      "Epoch 28/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9770 - val_loss: 0.1592 - val_accuracy: 0.9669\n",
      "Epoch 29/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9841 - val_loss: 0.1486 - val_accuracy: 0.9681\n",
      "Epoch 30/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9813 - val_loss: 0.2005 - val_accuracy: 0.9596\n",
      "Epoch 31/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9828 - val_loss: 0.1503 - val_accuracy: 0.9708\n",
      "Epoch 32/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9846 - val_loss: 0.1574 - val_accuracy: 0.9673\n",
      "Epoch 33/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9844 - val_loss: 0.1475 - val_accuracy: 0.9669\n",
      "Epoch 34/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0507 - accuracy: 0.9871 - val_loss: 0.1415 - val_accuracy: 0.9669\n",
      "Epoch 35/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9791 - val_loss: 0.1565 - val_accuracy: 0.9693\n",
      "Epoch 36/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9851 - val_loss: 0.1405 - val_accuracy: 0.9712\n",
      "Epoch 37/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9887 - val_loss: 0.1694 - val_accuracy: 0.9685\n",
      "Epoch 38/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9852 - val_loss: 0.1707 - val_accuracy: 0.9643\n",
      "Epoch 39/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9881 - val_loss: 0.1580 - val_accuracy: 0.9681\n",
      "Epoch 40/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9884 - val_loss: 0.1368 - val_accuracy: 0.9731\n",
      "Epoch 41/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9873 - val_loss: 0.1418 - val_accuracy: 0.9719\n",
      "Epoch 42/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9866 - val_loss: 0.1643 - val_accuracy: 0.9681\n",
      "Epoch 43/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.9879 - val_loss: 0.1671 - val_accuracy: 0.9708\n",
      "Epoch 44/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9895 - val_loss: 0.1476 - val_accuracy: 0.9723\n",
      "Epoch 45/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9885 - val_loss: 0.1847 - val_accuracy: 0.9558\n",
      "Epoch 46/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9875 - val_loss: 0.1474 - val_accuracy: 0.9731\n",
      "Epoch 47/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9894 - val_loss: 0.1344 - val_accuracy: 0.9743\n",
      "Epoch 48/50\n",
      "326/326 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9918 - val_loss: 0.1555 - val_accuracy: 0.9716\n",
      "Epoch 49/50\n",
      "326/326 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9914 - val_loss: 0.1362 - val_accuracy: 0.9758\n",
      "Epoch 50/50\n",
      "326/326 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9918 - val_loss: 0.1494 - val_accuracy: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faa48877e90>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=50, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10408, 12)         10572     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10408, 8)          104       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10408, 8)          72        \n",
      "=================================================================\n",
      "Total params: 10,748\n",
      "Trainable params: 10,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/thivakkarmahendran/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/thivakkarmahendran/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: Trained Model/assets\n"
     ]
    }
   ],
   "source": [
    "#save the model t\n",
    "model.save('Trained Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
